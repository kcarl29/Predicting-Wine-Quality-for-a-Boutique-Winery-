# -*- coding: utf-8 -*-
"""
Predicting Wine Quality for a Boutique Winery

To run:
1. Make sure you have installed the required packages:
   pip install gradio scikit-learn pandas numpy matplotlib seaborn
2. Place your dataset (e.g., 'winequality-red-selected-missing.csv') in the same directory.
"""

# ===================================================================
# SECTION 1: SETUP AND DATA LOADING
# ===================================================================

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve
from sklearn.impute import SimpleImputer
import gradio as gr
import pickle
import warnings
warnings.filterwarnings('ignore')

print("üç∑ Wine Quality Prediction System")
print("=" * 50)
print("Setting up environment for Mr. Sanborn's Quality Assurance Team")

# ===================================================================
# SECTION 2: DATA LOADING AND EXPLORATION
# ===================================================================

def load_and_explore_data():
    """Load the wine dataset and perform initial exploration"""
    # Attempt loading different file formats
    for ext in ['.csv', '.txt', '.data']:
        try:
            if ext == '.csv':
                df = pd.read_csv(f'winequality-red-selected-missing{ext}')
            else:
                df = pd.read_csv(f'winequality-red-selected-missing{ext}', sep=';' if ext == '.txt' else None)
            print(f"‚úÖ Successfully loaded dataset: winequality-red-selected-missing{ext}")
            break
        except Exception:
            df = None
    if df is None:
        print("‚ùå Dataset not found. Please upload 'winequality-red-selected-missing' file to this directory.")
        return None

    print(f"\nüìä Dataset Overview:")
    print(f"Shape: {df.shape}")
    print(f"Columns: {list(df.columns)}")

    print(f"\nüìà Dataset Info:")
    print(df.info())

    print(f"\nüîç First 5 rows:")
    print(df.head())

    print(f"\n‚ùì Missing Values Analysis:")
    missing_data = df.isnull().sum()
    missing_percent = (missing_data / len(df)) * 100
    missing_df = pd.DataFrame({
        'Missing Count': missing_data,
        'Missing Percentage': missing_percent.round(2)
    })
    missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)
    print(missing_df)

    print(f"\nüéØ Quality Distribution:")
    quality_dist = df['quality'].value_counts().sort_index()
    print(quality_dist)

    return df

df = load_and_explore_data()

# ===================================================================
# SECTION 3: DATA PREPROCESSING AND FEATURE ENGINEERING
# ===================================================================

def preprocess_wine_data(df):
    """Comprehensive data preprocessing for wine quality prediction"""
    if df is None:
        return None, None, None, None, None, None

    print("\nüîß Starting Data Preprocessing...")
    data = df.copy()
    data['is_good_quality'] = (data['quality'] >= 7).astype(int)
    print(f"Quality Distribution:")
    print(f"Good Quality (‚â•7): {data['is_good_quality'].sum()} samples ({data['is_good_quality'].mean()*100:.1f}%)")
    print(f"Not Good Quality (<7): {(1-data['is_good_quality']).sum()} samples ({(1-data['is_good_quality'].mean())*100:.1f}%)")

    feature_columns = [col for col in data.columns if col not in ['quality', 'is_good_quality']]
    X = data[feature_columns]
    y = data['is_good_quality']

    print(f"\nüîÑ Handling Missing Values...")
    imputer = SimpleImputer(strategy='mean')
    X_imputed = pd.DataFrame(
        imputer.fit_transform(X),
        columns=X.columns,
        index=X.index
    )
    print("‚úÖ Missing values handled using mean imputation")

    X_train, X_test, y_train, y_test = train_test_split(
        X_imputed, y, test_size=0.2, random_state=42, stratify=y
    )

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    print(f"‚úÖ Data split into training ({len(X_train)}) and testing ({len(X_test)}) sets")
    print("‚úÖ Features scaled using StandardScaler")

    return X_train_scaled, X_test_scaled, y_train, y_test, imputer, scaler

X_train, X_test, y_train, y_test, imputer, scaler = preprocess_wine_data(df)

# ===================================================================
# SECTION 4: DATA VISUALIZATION
# ===================================================================

def create_visualizations(df):
    """Create comprehensive visualizations for wine quality analysis"""
    if df is None:
        return

    print("\nüìä Creating Data Visualizations...")
    plt.style.use('default')
    fig = plt.figure(figsize=(20, 15))

    plt.subplot(3, 4, 1)
    df['quality'].hist(bins=6, alpha=0.7, color='darkred', edgecolor='black')
    plt.title('Wine Quality Distribution', fontsize=12, fontweight='bold')
    plt.xlabel('Quality Rating')
    plt.ylabel('Frequency')

    plt.subplot(3, 4, 2)
    sns.heatmap(df.isnull(), cbar=True, cmap='Reds', yticklabels=False)
    plt.title('Missing Values Pattern', fontsize=12, fontweight='bold')

    plt.subplot(3, 4, 3)
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    corr_matrix = df[numeric_cols].corr()
    sns.heatmap(corr_matrix, annot=False, cmap='RdYlBu', center=0, square=True, cbar=False)
    plt.title('Feature Correlation Matrix', fontsize=12, fontweight='bold')

    feature_plots = ['alcohol', 'volatile acidity', 'sulphates', 'citric acid']
    for i, feature in enumerate(feature_plots[:4], 4):
        plt.subplot(3, 4, i)
        if feature in df.columns:
            df.boxplot(column=feature, by='quality', ax=plt.gca())
            plt.title(f'Quality vs {feature.title()}')
            plt.suptitle('')
        else:
            similar_cols = [col for col in df.columns if feature.replace(' ', '_') in col or feature.replace(' ', '.') in col]
            if similar_cols:
                df.boxplot(column=similar_cols[0], by='quality', ax=plt.gca())
                plt.title(f'Quality vs {similar_cols[0]}')
                plt.suptitle('')

    key_features = df.select_dtypes(include=[np.number]).columns[:4]
    for i, feature in enumerate(key_features, 8):
        if i > 11:
            break
        plt.subplot(3, 4, i)
        df[feature].hist(bins=20, alpha=0.7, color='darkred', edgecolor='black')
        plt.title(f'{feature} Distribution')
        plt.xlabel(feature)
        plt.ylabel('Frequency')

    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 3, 1)
    binary_quality = (df['quality'] >= 7).astype(int)
    binary_quality.value_counts().plot(kind='bar', color=['lightcoral', 'darkgreen'])
    plt.title('Binary Quality Classification', fontweight='bold')
    plt.xlabel('Quality (0: Not Good, 1: Good)')
    plt.ylabel('Count')
    plt.xticks(rotation=0)

    plt.subplot(1, 3, 2)
    df['quality'].hist(bins=6, alpha=0.7, color='darkred', edgecolor='black')
    plt.axvline(x=7, color='green', linestyle='--', linewidth=2, label='Good Quality Threshold')
    plt.title('Quality Distribution with Threshold', fontweight='bold')
    plt.xlabel('Quality Rating')
    plt.ylabel('Frequency')
    plt.legend()

    plt.subplot(1, 3, 3)
    missing_data = df.isnull().sum()
    missing_data = missing_data[missing_data > 0].sort_values(ascending=False)
    if len(missing_data) > 0:
        missing_data.plot(kind='bar', color='orange')
        plt.title('Missing Values by Feature', fontweight='bold')
        plt.xlabel('Features')
        plt.ylabel('Missing Count')
        plt.xticks(rotation=45)
    else:
        plt.text(0.5, 0.5, 'No Missing Values Found', ha='center', va='center', fontsize=14)
        plt.title('Missing Values by Feature', fontweight='bold')
    plt.tight_layout()
    plt.show()

create_visualizations(df)

# ===================================================================
# SECTION 5: MODEL TRAINING AND EVALUATION
# ===================================================================

def train_and_evaluate_models(X_train, X_test, y_train, y_test):
    """Train multiple models and compare their performance"""
    print("\nü§ñ Training Machine Learning Models...")
    models = {
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100)
    }
    results = {}
    for name, model in models.items():
        print(f"\nüìà Training {name}...")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]
        accuracy = accuracy_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_prob)
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'roc_auc': roc_auc,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'predictions': y_pred,
            'probabilities': y_prob
        }
        print(f"‚úÖ {name} Results:")
        print(f"   Test Accuracy: {accuracy:.4f}")
        print(f"   ROC AUC: {roc_auc:.4f}")
        print(f"   CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    return results

if X_train is not None:
    model_results = train_and_evaluate_models(X_train, X_test, y_train, y_test)
    best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['roc_auc'])
    best_model = model_results[best_model_name]['model']
    print(f"\nüèÜ Best Model: {best_model_name}")
    print(f"ROC AUC Score: {model_results[best_model_name]['roc_auc']:.4f}")

# ===================================================================
# SECTION 6: MODEL EVALUATION AND VISUALIZATION
# ===================================================================

def evaluate_model_performance(model_results, y_test):
    """Create comprehensive model evaluation visualizations"""
    print("\nüìä Creating Model Evaluation Visualizations...")
    fig = plt.figure(figsize=(15, 10))

    plt.subplot(2, 3, 1)
    model_names = list(model_results.keys())
    accuracies = [model_results[name]['accuracy'] for name in model_names]
    roc_aucs = [model_results[name]['roc_auc'] for name in model_names]
    x = np.arange(len(model_names))
    width = 0.35
    plt.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.7)
    plt.bar(x + width/2, roc_aucs, width, label='ROC AUC', alpha=0.7)
    plt.title('Model Performance Comparison', fontweight='bold')
    plt.xlabel('Models')
    plt.ylabel('Score')
    plt.xticks(x, model_names)
    plt.legend()
    plt.ylim(0, 1)

    plt.subplot(2, 3, 2)
    best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['roc_auc'])
    y_pred = model_results[best_model_name]['predictions']
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', square=True)
    plt.title(f'Confusion Matrix - {best_model_name}', fontweight='bold')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

    plt.subplot(2, 3, 3)
    for name in model_results.keys():
        y_prob = model_results[name]['probabilities']
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc_score = model_results[name]['roc_auc']
        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')
    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    plt.title('ROC Curves', fontweight='bold')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()

    plt.subplot(2, 3, 4)
    if 'Random Forest' in model_results:
        rf_model = model_results['Random Forest']['model']
        feature_importance = rf_model.feature_importances_
        feature_names = [col for col in df.columns if col not in ['quality', 'is_good_quality']][:len(feature_importance)]
        indices = np.argsort(feature_importance)[::-1][:10]
        plt.bar(range(len(indices)), feature_importance[indices])
        plt.title('Top 10 Feature Importance (Random Forest)', fontweight='bold')
        plt.xlabel('Features')
        plt.ylabel('Importance')
        plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=45)

    plt.subplot(2, 3, 5)
    cv_means = [model_results[name]['cv_mean'] for name in model_names]
    cv_stds = [model_results[name]['cv_std'] for name in model_names]
    plt.bar(model_names, cv_means, yerr=cv_stds, alpha=0.7, capsize=5)
    plt.title('Cross-Validation Accuracy', fontweight='bold')
    plt.xlabel('Models')
    plt.ylabel('Accuracy')
    plt.ylim(0, 1)

    plt.subplot(2, 3, 6)
    y_prob = model_results[best_model_name]['probabilities']
    prob_good = y_prob[y_test == 1]
    prob_not_good = y_prob[y_test == 0]
    plt.hist(prob_not_good, bins=20, alpha=0.7, label='Not Good (Actual)', color='red')
    plt.hist(prob_good, bins=20, alpha=0.7, label='Good (Actual)', color='green')
    plt.title(f'Prediction Probability Distribution\n{best_model_name}', fontweight='bold')
    plt.xlabel('Predicted Probability')
    plt.ylabel('Frequency')
    plt.legend()
    plt.tight_layout()
    plt.show()

    print(f"\nüìã Detailed Classification Report - {best_model_name}:")
    print(classification_report(y_test, y_pred, target_names=['Not Good Quality', 'Good Quality']))

if X_train is not None:
    evaluate_model_performance(model_results, y_test)

# ===================================================================
# SECTION 7: SAVE MODEL AND PREPROCESSING COMPONENTS
# ===================================================================

def save_model_components(best_model, imputer, scaler, feature_names):
    """Save the trained model and preprocessing components"""
    print("\nüíæ Saving Model Components...")
    model_package = {
        'model': best_model,
        'imputer': imputer,
        'scaler': scaler,
        'feature_names': feature_names
    }
    with open('wine_quality_model.pkl', 'wb') as f:
        pickle.dump(model_package, f)
    print("‚úÖ Model saved as 'wine_quality_model.pkl'")
    return model_package

if X_train is not None:
    feature_names = [col for col in df.columns if col not in ['quality', 'is_good_quality']]
    model_package = save_model_components(best_model, imputer, scaler, feature_names)

# ===================================================================
# SECTION 8: GRADIO WEB APPLICATION
# ===================================================================

def create_wine_quality_app():
    """Create a Gradio web application for wine quality prediction"""

    def predict_wine_quality(*inputs):
        """Make wine quality prediction"""
        try:
            global model_package
            if 'model_package' not in globals():
                with open('wine_quality_model.pkl', 'rb') as f:
                    model_package = pickle.load(f)
            feature_names = model_package['feature_names']
            input_data = np.array(inputs).reshape(1, -1)
            input_df = pd.DataFrame(input_data, columns=feature_names)
            input_imputed = model_package['imputer'].transform(input_df)
            input_scaled = model_package['scaler'].transform(input_imputed)
            prediction = model_package['model'].predict(input_scaled)[0]
            probability = model_package['model'].predict_proba(input_scaled)[0]
            quality_label = "Good Quality Wine (‚â•7)" if prediction == 1 else "Not Good Quality Wine (<7)"
            confidence = max(probability) * 100
            result = f"""
            üç∑ **Wine Quality Prediction Results**

            **Quality Classification:** {quality_label}

            **Confidence Score:** {confidence:.1f}%

            **Probability Breakdown:**
            - Not Good Quality: {probability[0]:.3f} ({probability[0]*100:.1f}%)
            - Good Quality: {probability[1]:.3f} ({probability[1]*100:.1f}%)

            **Interpretation:**
            {interpret_confidence(confidence, prediction)}

            **Recommendation for Mr. Sanborn's Team:**
            {get_recommendation(confidence, prediction)}
            """
            return result
        except Exception as e:
            return f"Error making prediction: {str(e)}"

    def interpret_confidence(confidence, prediction):
        """Interpret the confidence score"""
        quality = "good" if prediction == 1 else "not good"
        if confidence >= 90:
            return f"Very high confidence that this wine is {quality} quality. The model is very certain about this prediction."
        elif confidence >= 80:
            return f"High confidence that this wine is {quality} quality. The model is quite certain about this prediction."
        elif confidence >= 70:
            return f"Moderate confidence that this wine is {quality} quality. Consider additional testing for verification."
        elif confidence >= 60:
            return f"Low-moderate confidence that this wine is {quality} quality. Recommend expert evaluation."
        else:
            return f"Low confidence in the prediction. Strong recommendation for expert evaluation and additional testing."

    def get_recommendation(confidence, prediction):
        """Get business recommendation"""
        if prediction == 1:
            if confidence >= 80:
                return "‚úÖ Approve for premium wine selection. High confidence in quality."
            elif confidence >= 70:
                return "‚ö†Ô∏è Likely suitable for premium selection, but consider sensory evaluation."
            else:
                return "üîç Requires expert tasting and additional quality assessment before approval."
        else:
            if confidence >= 80:
                return "‚ùå Not suitable for premium wine selection. High confidence in classification."
            elif confidence >= 70:
                return "‚ö†Ô∏è Likely not suitable for premium selection, but consider expert review."
            else:
                return "üîç Uncertain classification. Recommend expert evaluation to determine quality level."

    if df is not None:
        feature_names = [col for col in df.columns if col not in ['quality', 'is_good_quality']]
        inputs = []
        for feature in feature_names:
            if feature in df.columns:
                default_val = df[feature].mean()
                min_val = df[feature].min()
                max_val = df[feature].max()
            else:
                default_val, min_val, max_val = 0.5, 0.0, 1.0
            inputs.append(
                gr.Number(
                    value=default_val,
                    minimum=min_val,
                    maximum=max_val,
                    label=feature.replace('_', ' ').title(),
                    info=f"Range: {min_val:.2f} - {max_val:.2f}"
                )
            )
        interface = gr.Interface(
            fn=predict_wine_quality,
            inputs=inputs,
            outputs=gr.Markdown(label="Prediction Results"),
            title="üç∑ Wine Quality Prediction System",
            description="""
            ## Boutique Winery Quality Assessment Tool
            **For Mr. Sanborn's Quality Assurance Team**

            Enter the chemical analysis values for a wine sample to get an instant quality prediction.
            The system automatically handles missing values using statistical imputation.

            **Quality Definition:**
            - **Good Quality**: Rating ‚â• 7 (Premium wine selection)
            - **Not Good Quality**: Rating < 7 (Standard wine selection)
            """,
            examples=[
                [7.4, 0.7, 0.0, 1.9, 0.076, 11, 34, 0.9978, 3.51, 0.56, 9.4],
                [7.8, 0.88, 0.0, 2.6, 0.098, 25, 67, 0.9968, 3.2, 0.68, 9.8]
            ],
            theme=gr.themes.Soft(),
            allow_flagging="never"
        )
        return interface
    else:
        return None

print("\nüåê Creating Web Application...")

if 'model_package' in globals():
    app = create_wine_quality_app()
    if app is not None:
        print("‚úÖ Web application created successfully!")
        print("üöÄ Launching application...")
        app.launch(
            share=True,
            debug=True,
            server_name="0.0.0.0",
            server_port=7860
        )
    else:
        print("‚ùå Could not create web application. Please ensure dataset is loaded properly.")
else:
    print("‚ùå Model not trained. Please ensure dataset is loaded and models are trained first.")

# ===================================================================
# SECTION 9: BUSINESS REPORTING
# ===================================================================

def generate_business_report(df, model_results):
    """Generate a comprehensive business report for Mr. Sanborn"""
    print("\nüìä WINE QUALITY PREDICTION SYSTEM - BUSINESS REPORT")
    print("=" * 60)
    print("Prepared for: Mr. Sanborn, Head of Quality Assurance")
    print("Boutique Winery Quality Assessment")
    print("=" * 60)

    if df is not None and model_results is not None:
        print(f"\n1. DATASET OVERVIEW:")
        print(f"   ‚Ä¢ Total wine samples analyzed: {len(df)}")
        print(f"   ‚Ä¢ Features analyzed: {len([col for col in df.columns if col not in ['quality', 'is_good_quality']])}")
        good_quality_count = (df['quality'] >= 7).sum()
        good_quality_pct = (good_quality_count / len(df)) * 100
        print(f"   ‚Ä¢ Good quality wines (‚â•7): {good_quality_count} ({good_quality_pct:.1f}%)")
        print(f"   ‚Ä¢ Standard quality wines (<7): {len(df) - good_quality_count} ({100-good_quality_pct:.1f}%)")
        missing_data = df.isnull().sum().sum()
        missing_pct = (missing_data / (len(df) * len(df.columns))) * 100
        print(f"\n2. DATA QUALITY ASSESSMENT:")
        print(f"   ‚Ä¢ Missing values handled: {missing_data} ({missing_pct:.1f}% of total data)")
        print(f"   ‚Ä¢ Imputation strategy: Statistical mean replacement")
        print(f"   ‚Ä¢ Data preprocessing: Standardization applied")
        best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['roc_auc'])
        best_performance = model_results[best_model_name]
        print(f"\n3. MODEL PERFORMANCE:")
        print(f"   ‚Ä¢ Best performing model: {best_model_name}")
        print(f"   ‚Ä¢ Accuracy: {best_performance['accuracy']:.1%}")
        print(f"   ‚Ä¢ ROC AUC Score: {best_performance['roc_auc']:.3f}")
        print(f"   ‚Ä¢ Cross-validation accuracy: {best_performance['cv_mean']:.1%} ¬± {best_performance['cv_std']*2:.1%}")
        print(f"\n4. BUSINESS RECOMMENDATIONS:")
        if best_performance['accuracy'] >= 0.85:
            print(f"   ‚úÖ EXCELLENT: Model performance exceeds industry standards")
            print(f"   ‚Ä¢ Ready for production deployment")
            print(f"   ‚Ä¢ Can reliably support quality assurance decisions")
        elif best_performance['accuracy'] >= 0.75:
            print(f"   ‚úÖ GOOD: Model performance is suitable for business use")
            print(f"   ‚Ä¢ Recommend deployment with expert oversight")
            print(f"   ‚Ä¢ Use confidence scores to flag uncertain cases")
        else:
            print(f"   ‚ö†Ô∏è CAUTION: Model performance needs improvement")
            print(f"   ‚Ä¢ Consider collecting more data")
            print(f"   ‚Ä¢ Use as supplementary tool only")
        print(f"\n5. DEPLOYMENT STRATEGY:")
        print(f"   ‚Ä¢ Web application: Available for immediate use")
        print(f"   ‚Ä¢ Input handling: Automatic missing value compensation")
        print(f"   ‚Ä¢ Output format: Binary classification with confidence scores")
        print(f"   ‚Ä¢ Integration: Ready for quality assurance workflow")
        print(f"\n6. RISK MITIGATION:")
        print(f"   ‚Ä¢ Confidence threshold: Review cases below 70% confidence")
        print(f"   ‚Ä¢ Expert validation: Recommended for borderline cases")
        print(f"   ‚Ä¢ Continuous monitoring: Track prediction accuracy over time")
        print(f"   ‚Ä¢ Model updates: Retrain with new data quarterly")
    else:
        print("‚ùå Cannot generate report - missing data or model results")
